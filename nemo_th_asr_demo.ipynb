{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune ASR tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Thai Common Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-6.1-2020-12-11/th.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -xvf th.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import pythainlp\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import normalize, isthaichar\n",
    "import multiprocessing as mp\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create menifest for NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeMo menifest format <br>\n",
    "```json\n",
    "{\n",
    "    \"audio_filepath\": \"cv-corpus-6.1-2020-12-11/th/clips/common_voice_th_23656065_trim.wav\",\n",
    "    \"text\": \"เขา#กำลัง#มา#พอดี\", \n",
    "    \"duration\": 1.253877551020408\n",
    "}\n",
    "{\n",
    "    \"audio_filepath\": \"cv-corpus-6.1-2020-12-11/th/clips/common_voice_th_23727063_trim.wav\",\n",
    "    \"text\": \"ฉัน#สามารถ#ทำ#อะไร#ให้#คุณ#ได้#บ้าง\", \n",
    "    \"duration\": 4.2260317460317465\n",
    "}\n",
    "```\n",
    "\n",
    "We use # instead of spaces because the tool for build language models is word-level language models and it uses spaces to separate tokens, but we want to use character-level language models, so we need to use different symbols instead of spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    norm_text = normalize(text)\n",
    "    norm_text = ''.join(char for char in norm_text if isthaichar(char))\n",
    "    words = word_tokenize(norm_text, engine='attacut')\n",
    "    words = [words[i-1] if words[i] == 'ๆ' else words[i] for i in range(len(words))]\n",
    "    return '#'.join(words)\n",
    "\n",
    "def __process(inp):\n",
    "    filename, transcript, inp_dir, out_dir = inp\n",
    "    inp_file_path = os.path.join(inp_dir, \"clips\", filename)\n",
    "    out_file_path = os.path.join(out_dir, \"clips\", filename.replace('.mp3', '_trim.wav'))\n",
    "    text = clean_text(transcript)\n",
    "    y, sr = librosa.load(inp_file_path)\n",
    "    y_trim, _ = librosa.effects.trim(y, top_db=30)\n",
    "    duration = librosa.get_duration(y_trim, sr)\n",
    "    wavfile.write(out_file_path, sr, (y_trim*32768).astype('int16'))\n",
    "    return '{\"audio_filepath\": \"'+out_file_path+'\", \\\n",
    "                       \"text\": \"'+text.strip()+'\", \"duration\": '+str(duration)+'}\\n'\n",
    "\n",
    "def clean_and_build_menifest(inp_dir, out_dir, num_workers = 30):\n",
    "\n",
    "    cates = ['train', 'dev', 'test']\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(os.path.join(out_dir, 'clips'))\n",
    "    \n",
    "    for cate in cates:\n",
    "        entries = []\n",
    "        df = pd.read_csv(os.path.join(inp_dir, cate+\".tsv\"), sep='\\t')\n",
    "        with mp.Pool(num_workers) as p:\n",
    "            results = p.imap(__process, [(row.path, row.sentence, inp_dir, out_dir) \\\n",
    "                                         for row in df.itertuples(index=False)])\n",
    "\n",
    "            for result in tqdm.tqdm(results, total=len(df)):\n",
    "                entries.append(result)\n",
    "                # print(entries)\n",
    "\n",
    "        json_file = open(os.path.join(out_dir, cate+'_trim.json'), 'w')\n",
    "        for entry in entries:\n",
    "            json_file.write(entry)\n",
    "        json_file.close()\n",
    "\n",
    "# Create menifest\n",
    "num_workers = 16\n",
    "clean_and_build_menifest('cv-corpus-6.1-2020-12-11/th/', 'common_voice_th_clean', num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add word error rate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WER metric implemented in NeMo is only support seperate word by space but we use #, So we need to customize WER metric code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from nemo.utils import logging\n",
    "from typing import List\n",
    "from torchmetrics import Metric\n",
    "import editdistance\n",
    "\n",
    "class CUSTOM_WER(Metric):\n",
    "    def __init__(self, \n",
    "                 vocabulary,\n",
    "                 batch_dim_index=0,\n",
    "                 use_cer=False,\n",
    "                 ctc_decode=True,\n",
    "                 log_prediction=True,\n",
    "                 dist_sync_on_step=False,\n",
    "                 sep_token=' ',):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step, compute_on_step=False)\n",
    "        self.batch_dim_index = batch_dim_index\n",
    "        self.blank_id = len(vocabulary)\n",
    "        self.labels_map = dict([(i, vocabulary[i]) for i in range(len(vocabulary))])\n",
    "        self.use_cer = use_cer\n",
    "        self.ctc_decode = ctc_decode\n",
    "        self.log_prediction = log_prediction\n",
    "\n",
    "        self.add_state(\"scores\", default=torch.tensor(0), dist_reduce_fx='sum', persistent=False)\n",
    "        self.add_state(\"words\", default=torch.tensor(0), dist_reduce_fx='sum', persistent=False)\n",
    "        \n",
    "        self.sep_token=sep_token\n",
    "\n",
    "    def ctc_decoder_predictions_tensor(\n",
    "        self, predictions: torch.Tensor, predictions_len: torch.Tensor = None, return_hypotheses: bool = False,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decodes a sequence of labels to words\n",
    "        Args:\n",
    "            predictions: A torch.Tensor of shape [Batch, Time] of integer indices that correspond\n",
    "                to the index of some character in the label set.\n",
    "            predictions_len: Optional tensor of length `Batch` which contains the integer lengths\n",
    "                of the sequence in the padded `predictions` tensor.\n",
    "            return_hypotheses: Bool flag whether to return just the decoding predictions of the model\n",
    "                or a Hypothesis object that holds information such as the decoded `text`,\n",
    "                the `alignment` of emited by the CTC Model, and the `length` of the sequence (if available).\n",
    "                May also contain the log-probabilities of the decoder (if this method is called via\n",
    "                transcribe())\n",
    "        Returns:\n",
    "            Either a list of str which represent the CTC decoded strings per sample,\n",
    "            or a list of Hypothesis objects containing additional information.\n",
    "        \"\"\"\n",
    "        hypotheses = []\n",
    "        # Drop predictions to CPU\n",
    "        prediction_cpu_tensor = predictions.long().cpu()\n",
    "        # iterate over batch\n",
    "        for ind in range(prediction_cpu_tensor.shape[self.batch_dim_index]):\n",
    "            prediction = prediction_cpu_tensor[ind].detach().numpy().tolist()\n",
    "            if predictions_len is not None:\n",
    "                prediction = prediction[: predictions_len[ind]]\n",
    "            # CTC decoding procedure\n",
    "            decoded_prediction = []\n",
    "            previous = self.blank_id\n",
    "            for p in prediction:\n",
    "                if (p != previous or previous == self.blank_id) and p != self.blank_id:\n",
    "                    decoded_prediction.append(p)\n",
    "                previous = p\n",
    "\n",
    "            text = self.decode_tokens_to_str(decoded_prediction)\n",
    "\n",
    "            if not return_hypotheses:\n",
    "                hypothesis = text\n",
    "            else:\n",
    "                hypothesis = Hypothesis(\n",
    "                    y_sequence=None,\n",
    "                    score=-1.0,\n",
    "                    text=text,\n",
    "                    alignments=prediction,\n",
    "                    length=predictions_len[ind] if predictions_len is not None else 0,\n",
    "                )\n",
    "\n",
    "            hypotheses.append(hypothesis)\n",
    "        return hypotheses\n",
    "\n",
    "    def decode_tokens_to_str(self, tokens: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Implemented in order to decoder a token list into a string.\n",
    "        Args:\n",
    "            tokens: List of int representing the token ids.\n",
    "        Returns:\n",
    "            A decoded string.\n",
    "        \"\"\"\n",
    "        hypothesis = ''.join(self.decode_ids_to_tokens(tokens))\n",
    "        return hypothesis\n",
    "\n",
    "    def decode_ids_to_tokens(self, tokens: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Implemented in order to decode a token id list into a token list.\n",
    "        A token list is the string representation of each token id.\n",
    "        Args:\n",
    "            tokens: List of int representing the token ids.\n",
    "        Returns:\n",
    "            A list of decoded tokens.\n",
    "        \"\"\"\n",
    "        token_list = [self.labels_map[c] for c in tokens if c != self.blank_id]\n",
    "        return token_list\n",
    "\n",
    "    def compute(self):\n",
    "        scores = self.scores.detach().float()\n",
    "        words = self.words.detach().float()\n",
    "        return scores / words, scores, words\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        predictions: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        target_lengths: torch.Tensor,\n",
    "        predictions_lengths: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        words = 0.0\n",
    "        scores = 0.0\n",
    "        references = []\n",
    "        with torch.no_grad():\n",
    "            # prediction_cpu_tensor = tensors[0].long().cpu()\n",
    "            targets_cpu_tensor = targets.long().cpu()\n",
    "            tgt_lenths_cpu_tensor = target_lengths.long().cpu()\n",
    "\n",
    "            # iterate over batch\n",
    "            for ind in range(targets_cpu_tensor.shape[self.batch_dim_index]):\n",
    "                tgt_len = tgt_lenths_cpu_tensor[ind].item()\n",
    "                target = targets_cpu_tensor[ind][:tgt_len].numpy().tolist()\n",
    "                reference = self.decode_tokens_to_str(target)\n",
    "                references.append(reference)\n",
    "            if self.ctc_decode:\n",
    "                hypotheses = self.ctc_decoder_predictions_tensor(predictions, predictions_lengths)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Implement me if you need non-CTC decode on predictions\")\n",
    "\n",
    "        if self.log_prediction:\n",
    "            logging.info(f\"\\n\")\n",
    "            logging.info(f\"reference:{references[0]}\")\n",
    "            logging.info(f\"predicted:{hypotheses[0]}\")\n",
    "\n",
    "        for h, r in zip(hypotheses, references):\n",
    "            if self.use_cer:\n",
    "                h_list = list(h.replace(self.sep_token, ''))\n",
    "                r_list = list(r.replace(self.sep_token, ''))\n",
    "            else:\n",
    "                h_list = h.split(self.sep_token)\n",
    "                r_list = r.split(self.sep_token)\n",
    "            words += len(r_list)\n",
    "            # Compute Levenstein's distance\n",
    "            scores += editdistance.eval(h_list, r_list)\n",
    "\n",
    "        self.scores = torch.tensor(scores, device=self.scores.device, dtype=self.scores.dtype)\n",
    "        self.words = torch.tensor(words, device=self.words.device, dtype=self.words.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "import nemo\n",
    "from nemo.collections.asr.models import EncDecCTCModel\n",
    "from nemo.collections.common.callbacks import LogEpochTimeCallback\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.exp_manager import exp_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "cfg = OmegaConf.load('quartznet_5x3_th.yaml')\n",
    "\n",
    "# load pretrain\n",
    "asr_model = EncDecCTCModel.load_from_checkpoint(\"QuartzNet5x3_2e02_libri---val_wer=0.08-epoch=97.ckpt\")\n",
    "\n",
    "# change decoder output\n",
    "asr_model.change_vocabulary(\n",
    "        new_vocabulary=['#','ก','ข','ฃ','ค','ฅ','ฆ','ง','จ','ฉ','ช','ซ','ฌ','ญ','ฎ','ฏ','ฐ','ฑ','ฒ','ณ','ด','ต',\n",
    "                  'ถ','ท','ธ','น','บ','ป','ผ','ฝ','พ','ฟ','ภ','ม','ย','ร','ล','ว','ศ','ษ','ส','ห','ฬ',\n",
    "                  'อ','ฮ','ฤ','ฦ','ะ','ั','า','ำ','ิ','ี','ึ','ื','ุ','ู','เ','แ','โ','ใ','ไ','ๅ','ํ','็','่','้','๊','๋']\n",
    "    )\n",
    "\n",
    "# set metric\n",
    "asr_model._wer = CUSTOM_WER(\n",
    "            vocabulary=asr_model.decoder.vocabulary,\n",
    "            batch_dim_index=0,\n",
    "            use_cer=False,\n",
    "            ctc_decode=True,\n",
    "            dist_sync_on_step=True,\n",
    "            log_prediction=False,\n",
    "            sep_token='#'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(**cfg.trainer)\n",
    "exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "\n",
    "# setup dataset\n",
    "\n",
    "asr_model.setup_optimization(optim_config=DictConfig(cfg.model.optim))\n",
    "# Point to the data we'll use for fine-tuning as the training set\n",
    "asr_model.setup_training_data(train_data_config=cfg.model.train_ds)\n",
    "# Point to the new validation data for fine-tuning\n",
    "asr_model.setup_validation_data(val_data_config=cfg.model.validation_ds)\n",
    "\n",
    "asr_model.setup_test_data(test_data_config=cfg.model.test_ds)\n",
    "asr_model.set_trainer(trainer)\n",
    "\n",
    "trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(\"common_voice_th_clean/clips/common_voice_th_23670069_trim.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3406a4a5b734eb080f3b57cfeceb7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['เมื่อ#ฉัน#รัก#ใคร#ทุก#คน#อย่าง#อดโยน#และ#มัน#ดูก#จับ#ใซ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.transcribe([\n",
    "    \"common_voice_th_clean/clips/common_voice_th_23670069_trim.wav\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use KenLM to build language model.\n",
    "For more information about KenLM : https://github.com/kpu/kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transcripts = list()\n",
    "with open('common_voice_th_clean/test_trim.json', 'r') as test_manifest_file:\n",
    "    audio_file_paths = []\n",
    "    lines = test_manifest_file.read().strip('\\n').split('\\n')\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        target_transcripts.append(data['text'])\n",
    "        audio_file_paths.append(data['audio_filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.modules import BeamSearchDecoderWithLM\n",
    "from nemo.collections.asr.metrics.wer import WER, word_error_rate\n",
    "import kenlm_utils\n",
    "\n",
    "def beam_search(logits, beam_search_decoder, beam_batch_size=32):\n",
    "    hypotheses = list()\n",
    "    it = range(int(np.ceil(len(logits) / beam_batch_size)))\n",
    "    for batch_idx in it:\n",
    "        # disabling type checking\n",
    "        with nemo.core.typecheck.disable_checks():\n",
    "            probs_batch = logits[batch_idx * beam_batch_size : (batch_idx + 1) * beam_batch_size]\n",
    "            beams_batch = beam_search_lm.forward(log_probs=probs_batch, log_probs_length=None,)\n",
    "            for beams_idx, beams in enumerate(beams_batch):\n",
    "                hypotheses.append(beams[0][1])\n",
    "    return hypotheses\n",
    "\n",
    "def decode_with_lm(asr_model, decoder, files, batch_size=1):\n",
    "    logits = asr_model.transcribe(\n",
    "                files,\n",
    "                batch_size=batch_size,\n",
    "                logprobs=True\n",
    "            )\n",
    "    probs = [kenlm_utils.softmax(logit.cpu().numpy()) for logit in logits]\n",
    "    preds = beam_search(probs, beam_search_lm, batch_size)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial decoder\n",
    "beam_search_lm = BeamSearchDecoderWithLM(\n",
    "        vocab=asr_model.decoder.vocabulary,\n",
    "        beam_width=32,\n",
    "        alpha=2.0,\n",
    "        beta=1.7,\n",
    "        lm_path='common_voice_6gram.bin',\n",
    "        num_cpus=16,\n",
    "        input_tensor=False,\n",
    ")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_preds = decode_with_lm(asr_model, beam_search_lm, \n",
    "                            audio_file_paths, batch_size = 32)\n",
    "\n",
    "print(\"Inferrence Time: \", time.time()-start_time)\n",
    "\n",
    "test_cer_value = word_error_rate(hypotheses=[g.replace('#', '') for g in test_preds], \n",
    "                                 references=[g.replace('#', '') for g in target_transcripts], \n",
    "                                 use_cer=True)\n",
    "test_wer_value = word_error_rate(hypotheses=[g.replace('#', ' ') for g in test_preds], \n",
    "                                references=[g.replace('#', ' ') for g in target_transcripts])\n",
    "print(\"CER : {:.2f}%, WER : {:.2f}%\".format(test_cer_value*100, test_wer_value*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_with_lm(asr_model, \n",
    "               beam_search_lm, \n",
    "              [\"common_voice_th_clean/clips/common_voice_th_23670069_trim.wav\"]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
